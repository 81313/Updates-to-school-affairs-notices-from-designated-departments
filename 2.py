import requests
from bs4 import BeautifulSoup
import os
import time
from datetime import datetime
import urllib3
from pathlib import Path

# é—œé–‰ SSL è­¦å‘Šé¡¯ç¤ºï¼ˆå…è¨±ç•¥é SSL é©—è­‰ï¼‰
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- ç¶²ç«™è¨­å®š ---
WEBSITE_CONFIGS = [
    {
        # è‡ªå‹•åŒ–ç³»
        'url': 'https://autoweb.nfu.edu.tw/',
        'domain_name': 'NFU_AUTO',
        'parent_selector': 'div.fusion-recent-posts-2',
        'article_selector': 'article.post',
        'title_selector': 'div.recent-posts-content h4 a',
        'date_selector': 'div.recent-posts-content p.meta span:first-child',
        'max_items': 10
    },
    {
        # å­¸å‹™è™•
        'url': 'https://nfuosa.nfu.edu.tw/life.html',
        'domain_name': 'NFU_OSA',
        'parent_selector': 'table.category.table',
        'article_selector': 'tbody tr[class*="cat-list-row"]',
        'title_selector': 'a',
        'date_selector': 'td:nth-child(2)',
        'max_items': 10
    },
    {
        # ç¸½å‹™è™•
        'url': 
        'https://gaw.nfu.edu.tw/category/%e6%9c%80%e6%96%b0%e6%b6%88%e6%81%af/',
        'domain_name': 'NFU_GAW',
        'parent_selector': 
        'div.col.mainSection.mainSection-col-two.mainSection-pos-right#main',
        'article_selector': 'article.media',
        'title_selector': 'h1.media-heading.entry-title a',
        'date_selector': 'span.published.entry-meta_items',
        'max_items': 10
    },
    {
        # æ•™å‹™è™•
        'url': 'https://nfuacademic.nfu.edu.tw/',
        'domain_name': 'NFU_ACADEMIC',
        'parent_selector': 'div.wp-block-gutena-tab.active',
        'article_selector': 'li.wp-block-post.post',
        'title_selector': 'h3.wp-block-post-title a, h3.wp-block-post-title',
        'date_selector': 'time[datetime]',
        'max_items': 10
    }
]

# éœ€ç•¥é SSL æ†‘è­‰é©—è­‰çš„ç¶²åŸŸ
SSL_BYPASS_DOMAINS = [
    'autoweb.nfu.edu.tw',
    'nfuosa.nfu.edu.tw',
    'gaw.nfu.edu.tw',
    'nfuacademic.nfu.edu.tw'
]

def scrape_website_to_html(config):
    """
    çµ±ä¸€å…¬å‘Šæ ¼å¼ä¸¦è¼¸å‡ºè‡³æŒ‡å®šåœ°é»
    æ¯æ¬¡åŸ·è¡Œè‡ªå‹•è¦†è“‹èˆŠæª”
    """
    url = config['url']
    domain_name = config['domain_name']
    parent_selector = config['parent_selector']
    article_selector = config['article_selector']
    title_selector = config['title_selector']
    date_selector = config['date_selector']
    max_items = config['max_items']

    # è¼¸å‡ºç›®éŒ„èˆ‡æª”æ¡ˆè¨­å®š
    script_dir = Path(__file__).resolve().parent
    output_dir = script_dir / "public"
    output_dir.mkdir(parents=True, exist_ok=True)
    html_filename = output_dir / f"{domain_name}.html"

    # åˆªé™¤èˆŠæª”æ¡ˆï¼ˆä¿æŒæœ€æ–°ï¼‰
    for f in os.listdir(output_dir):
        if f.startswith(domain_name) and f.endswith(".html"):
            try:
                os.remove(os.path.join(output_dir, f))
            except Exception:
                pass

    print(f"\n--- ğŸŒ é–‹å§‹è™•ç†ç¶²ç«™: {domain_name} ({url}) ---")

    headers = {
        'User-Agent':
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
        'AppleWebKit/537.36 (KHTML, like Gecko)'
        'Chrome/100.0.4896.60'
        'Safari/537.36'
    }

    # SSL é©—è­‰æ§åˆ¶
    verify_ssl = not any(d in url for d in SSL_BYPASS_DOMAINS)

    try:
        response = requests.get(url, headers=headers, timeout=15, verify=verify_ssl)
        response.raise_for_status()
        response.encoding = 'utf-8'
    except requests.exceptions.RequestException as e:
        print(f"âŒ ç¶²ç«™è«‹æ±‚å¤±æ•—: {e}")
        return

    soup = BeautifulSoup(response.text, 'html.parser')

    # æ‰¾çˆ¶å®¹å™¨
    start_element = soup.select_one(parent_selector)
    if not start_element:
        print(f"âš ï¸ æ‰¾ä¸åˆ°çˆ¶å®¹å™¨: {parent_selector}")
        return

    # æ‰¾å…¬å‘Šé …ç›®
    announcement_items = start_element.select(article_selector)
    if not announcement_items:
        print(f"âš ï¸ æ‰¾ä¸åˆ°å…¬å‘Šé …ç›®: {article_selector}")
        return

    formatted_posts = []

    # è™•ç†å…¬å‘Šé …ç›®
    for i, item in enumerate(announcement_items[:max_items]):
        title_tag = item.select_one(title_selector)
        date_tag = item.select_one(date_selector)

        if not title_tag:
            continue

        title = title_tag.get_text(strip=True)
        link = title_tag.get('href', '#')
        date = date_tag.get_text(strip=True) if date_tag else "æœªæä¾›æ—¥æœŸ"

        formatted_html = f"""
        <div class="scraped-post-item" style="border-buttom: 2px solid black; padding: 6px; margin-bottom: 5px; font-family: 'DFKai-sb','Times New Roman';">
          <div class="scraped-header">
            <span class="scraped-source">ğŸ« {domain_name}</span>
            <span class="scraped-date">ğŸ“… {date}</span>
          </div>
          <div class="scraped-title">
            <a href="{link}" target="_blank">{title}</a>
          </div>
        </div>
        <hr class="announcement-separator">
        """
        formatted_posts.append(formatted_html)

    # è¼¸å‡º HTML
    if formatted_posts:
        try:
            with open(html_filename, 'w', encoding='utf-8-sig') as f:
                f.write(f'<div class="scraped-list-container unified-announcements">\n')
                for post in formatted_posts:
                    f.write(post)
                f.write('</div>\n')

            print(f"âœ… {domain_name} å®Œæˆï¼å…± {len(formatted_posts)} ç­†å…¬å‘Šï¼Œè¼¸å‡ºè‡³ï¼š{os.path.abspath(html_filename)}")
        except Exception as e:
            print(f"âŒ å¯«å…¥ HTML ç™¼ç”ŸéŒ¯èª¤: {e}")
    else:
        print(f"â„¹ï¸ {domain_name} æœªæŠ“å–åˆ°ä»»ä½•å…¬å‘Šã€‚")


# --- ä¸»ç¨‹å¼ ---
if __name__ == '__main__':
    print("ğŸ“¢ é–‹å§‹æ‰¹æ¬¡æŠ“å–å…¬å‘Šï¼ˆçµ±ä¸€æ ¼å¼ï¼‰...\n")
    for config in WEBSITE_CONFIGS:
        scrape_website_to_html(config)
        time.sleep(2)
    print("\nğŸ‰ å…¨éƒ¨ç¶²ç«™è™•ç†å®Œæˆï¼çµ±ä¸€å…¬å‘Šè¼¸å‡ºæ–¼ ./output ç›®éŒ„ã€‚")
